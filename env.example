# ============================================
# Transcription Backend Selection
# ============================================
# PROVIDERS_PRIORITY: Comma-separated list of providers in priority order
# The first available provider (with API key configured) becomes the default
# Available providers: openai, gemini, deepgram
# Default: openai,deepgram,gemini
# Example: PROVIDERS_PRIORITY=openai,gemini,deepgram
# PROVIDERS_PRIORITY=openai,deepgram,gemini

# ENABLE_DUMMY_PROVIDER: Enable the dummy backend for testing/statistics
# The dummy backend decodes audio but doesn't transcribe - useful for testing
# Default: false
# ENABLE_DUMMY_PROVIDER=true

# Runtime provider selection:
# You can override the default provider per request using the ?provider= URL parameter
# Examples:
#   ws://localhost:8080/transcribe?transcribe=true&sendBack=true&provider=gemini
#   ws://localhost:8080/transcribe?transcribe=true&sendBack=true&provider=deepgram
# If the requested provider is not available or invalid, the connection will be rejected

# ============================================
# OpenAI Configuration
# ============================================
# Required when TRANSCRIPTION_BACKEND=openai
OPENAI_API_KEY=sk-...

# OpenAI model to use for transcription
# Default: gpt-4o-mini-transcribe
# Options: gpt-4o-mini-transcribe, gpt-4o-transcribe
OPENAI_MODEL=gpt-4o-transcribe

# Custom transcription prompt for OpenAI
# Optional - helps guide the transcription model
OPENAI_TRANSCRIPTION_PROMPT="You will hear audio coming from one participant in a conference, your job is to transcribe it. Keep in mind the following: * There might be pauses during which another person speaks, you will not hear anything in this period * The person might speak any language, you need to detect it. However, it is unlikely that the language will switch often"

# Turn detection configuration (JSON)
# Default: {"type":"server_vad","threshold":0.5,"prefix_padding_ms":300,"silence_duration_ms":300}
# OPENAI_TURN_DETECTION={"type":"server_vad","threshold":0.5,"prefix_padding_ms":300,"silence_duration_ms":300}

# ============================================
# Google Gemini Configuration
# ============================================
# Required when TRANSCRIPTION_BACKEND=gemini
# GEMINI_API_KEY=your-gemini-api-key-here

# Gemini model to use for transcription
# Default: gemini-2.0-flash-exp
# Options: gemini-2.0-flash-exp (recommended for transcription), gemini-1.5-pro, gemini-1.5-flash
# Note: Native audio models (gemini-2.5-flash-native-audio-*) are for TTS/voice output, not transcription
# GEMINI_MODEL=gemini-2.0-flash-exp

# Custom transcription prompt for Gemini
# Optional - helps guide the transcription model
# GEMINI_TRANSCRIPTION_PROMPT="Transcribe the following audio. Output only the transcribed text."

# ============================================
# Deepgram Configuration
# ============================================
# Required when TRANSCRIPTION_BACKEND=deepgram
# DEEPGRAM_API_KEY=your-deepgram-api-key-here

# Deepgram model to use for transcription
# Default: nova-2
# Options: nova-2, nova, enhanced, base
# DEEPGRAM_MODEL=nova-2

# Audio encoding to use for Deepgram
# Default: linear16 (decoded PCM audio at 24kHz)
# Options:
#   - linear16: Send decoded PCM audio at 24kHz (default, more CPU but compatible)
#   - opus: Send raw Opus frames at 48kHz (less CPU, native Opus support)
# DEEPGRAM_ENCODING=linear16

# Language configuration for transcription
# Default: multi (automatic multilingual code-switching - 31+ languages with Nova-3)
# Options: multi, en, es, fr, de, pt, it, nl, ja, zh, ko, etc.
# Note: language=multi automatically detects and transcribes multiple languages in the same audio
# Note: detect_language parameter is NOT supported for streaming (only for pre-recorded audio)
# See: https://developers.deepgram.com/docs/multilingual-code-switching
# DEEPGRAM_LANGUAGE=multi

# Include detected language in transcript (e.g., "Hello [en]")
# Default: false
# DEEPGRAM_INCLUDE_LANGUAGE=true

# Enable automatic punctuation
# Default: false
# DEEPGRAM_PUNCTUATE=true

# Enable speaker diarization (identifies different speakers)
# Default: false
# DEEPGRAM_DIARIZE=false

# ============================================
# Server Configuration
# ============================================
# Port to listen on
# Default: 8080
PORT=8080

# Host to bind to
# Default: 0.0.0.0 (all interfaces)
HOST=0.0.0.0

# ============================================
# Transcription Configuration
# ============================================
# Force commit timeout in seconds (forces transcription when audio stops)
# Set to 0 to disable
# Default: 2
FORCE_COMMIT_TIMEOUT=2

# Broadcast transcripts to other participants as context
# When enabled, transcripts from other participants are added to each participant's
# transcription prompt to provide context
# Default: false
BROADCAST_TRANSCRIPTS=true

# Maximum size of transcript history in bytes
# Default: 5120 (5 KB)
BROADCAST_TRANSCRIPTS_MAX_SIZE=512

# ============================================
# Debugging and Development
# ============================================
# Dump WebSocket messages to files (for debugging/testing)
# Messages are written to DUMP_BASE_PATH/<session-id>/
# Default: false
DUMP_WEBSOCKET_MESSAGES=false

# Dump transcripts to files (for debugging/testing)
# Transcripts are written to DUMP_BASE_PATH/<session-id>/
# Default: false
DUMP_TRANSCRIPTS=false

# Base path for dumps
# Default: /tmp
DUMP_BASE_PATH=/tmp

# Log level: error, warn, info, debug
# Default: info
LOG_LEVEL=info

# Enable debug mode (verbose logging)
# Default: false
DEBUG=false

# ============================================
# Session Statistics
# ============================================
# Enable session statistics tracking and HTTP endpoints
# When enabled, provides real-time metrics via /stats and /stats/:sessionId endpoints
# Default: false
# ENABLE_SESSION_STATS=true
