import { Container, getContainer } from '@cloudflare/containers';
import { WorkerEntrypoint } from 'cloudflare:workers';

/**
 * Dispatcher message format for transcription events
 */
export interface DispatcherTranscriptionMessage {
	sessionId: string;
	endpointId: string;
	text: string;
	timestamp: number;
	language?: string;
}

/**
 * Response from dispatcher RPC call
 */
export interface RPCResponse {
	success: boolean;
	dispatched: number;
	errors?: string[];
	message?: string;
}

/**
 * Transcription Dispatcher worker interface
 */
export interface TranscriptionDispatcher extends WorkerEntrypoint<Env> {
	dispatch(message: DispatcherTranscriptionMessage): Promise<RPCResponse>;
}

/**
 * Transcription message from container
 */
interface TranscriptionMessage {
	type: 'transcription' | 'interim_transcription';
	participant: {
		id?: string;
	};
	transcript: Array<{
		text: string;
	}>;
	timestamp: number;
}

/**
 * TranscriberContainer wraps the Node.js transcription server
 * and forwards WebSocket requests to it.
 */
export class TranscriberContainer extends Container {
	// Port that the Node.js server listens on
	defaultPort = 8080;

	// How long to keep container running after last activity
	// After this period: Container goes to sleep (CPU/memory released)
	// Note: Cloudflare automatically manages sleep/wake - you cannot "destroy" containers
	// Shorter = More aggressive resource cleanup, but more wake-ups
	// Longer = Fewer cold starts, but uses more resources when idle
	// For pool-based routing: Keep this longer (containers serve many sessions)
	// For session-based routing: Keep this shorter (containers are session-specific)
	sleepAfter = '10m';

	// Pass environment variables to the container
	envVars = {
		// These will be available as process.env in the container
		OPENAI_API_KEY: this.env.OPENAI_API_KEY,
		OPENAI_MODEL: this.env.OPENAI_MODEL || 'gpt-4o-transcribe',
		FORCE_COMMIT_TIMEOUT: this.env.FORCE_COMMIT_TIMEOUT || '2',
		DEBUG: this.env.DEBUG || 'true',
		PORT: '8080',
		HOST: '0.0.0.0',
	};

	override onStart() {
		console.log('Transcriber container started');
	}

	override onStop() {
		console.log('Transcriber container stopped');
	}

	override onError(error: unknown) {
		console.error('Transcriber container error:', error);
	}
}

/**
 * Choose which container instance to route this request to.
 * Multiple routing strategies are available depending on your use case.
 */
async function selectContainerInstance(request: Request, env: Env): Promise<string> {
	const url = new URL(request.url);
	const sessionId = url.searchParams.get('sessionId');
	const routingMode = env.ROUTING_MODE || 'pool'; // 'pool', 'session', 'shared', or 'autoscale'

	switch (routingMode) {
		case 'autoscale':
			// Auto-scaling with coordinator: Automatically creates containers as needed
			// Use when: Variable traffic, want automatic scaling
			// Scaling: Dynamic based on load (coordinator manages)
			if (!sessionId) {
				throw new Error('sessionId required for autoscale mode');
			}
			const coordinator = env.CONTAINER_COORDINATOR.get(env.CONTAINER_COORDINATOR.idFromName('global'));
			const assignResponse = await coordinator.fetch(new URL('http://coordinator/assign'), {
				method: 'POST',
				headers: { 'Content-Type': 'application/json' },
				body: JSON.stringify({ sessionId }),
			});
			const { containerId } = await assignResponse.json<{ containerId: string }>();
			return containerId;

		case 'session':
			// Session affinity: Each sessionId gets its own container
			// Use when: You need persistent state per session
			// Scaling: One container per unique session (can be expensive)
			return sessionId || 'default';

		case 'shared':
			// Single shared container for all sessions
			// Use when: Low traffic, simple setup, minimal cold starts
			// Scaling: Fixed at 1 container
			return 'shared';

		case 'pool':
		default:
			// Pool-based routing: Distribute across a pool of containers
			// Use when: Many short-lived sessions, no per-session state needed
			// Scaling: Fixed pool size, containers stay warm, good for high throughput
			const poolSize = parseInt(env.CONTAINER_POOL_SIZE || '5', 10);

			// Hash the sessionId for consistent routing (optional)
			// If you want true randomness, use: Math.floor(Math.random() * poolSize)
			if (sessionId) {
				// Consistent hashing: Same sessionId â†’ Same container (within pool)
				let hash = 0;
				for (let i = 0; i < sessionId.length; i++) {
					hash = (hash << 5) - hash + sessionId.charCodeAt(i);
					hash = hash & hash; // Convert to 32bit integer
				}
				const poolIndex = Math.abs(hash) % poolSize;
				return `pool-${poolIndex}`;
			} else {
				// No sessionId: Random load balancing
				const poolIndex = Math.floor(Math.random() * poolSize);
				return `pool-${poolIndex}`;
			}
	}
}

export default {
	async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
		const url = new URL(request.url);

		// Handle stats endpoint for monitoring
		if (url.pathname === '/stats' && request.method === 'GET') {
			if (env.ROUTING_MODE === 'autoscale') {
				const coordinator = env.CONTAINER_COORDINATOR.get(env.CONTAINER_COORDINATOR.idFromName('global'));
				return coordinator.fetch(new URL('http://coordinator/stats'));
			} else {
				return Response.json({
					error: 'Stats only available in autoscale mode',
					routingMode: env.ROUTING_MODE || 'pool',
				});
			}
		}

		const useDispatcher = url.searchParams.get('useDispatcher') === 'true';
		const sessionId = url.searchParams.get('sessionId') || 'unknown';

		// Select which container instance to use based on routing strategy
		const containerInstanceId = await selectContainerInstance(request, env);

		// Get the container instance
		const container = getContainer(env.TRANSCRIBER, containerInstanceId);

		// For now, just forward all requests directly to the container
		// TODO: Implement WebSocket interception for dispatcher fan-out
		// This requires more complex WebSocket handling with Cloudflare Containers

		// Report connection tracking for autoscale mode
		const routingMode = env.ROUTING_MODE || 'pool';
		const upgradeHeader = request.headers.get('Upgrade');
		if (routingMode === 'autoscale' && upgradeHeader === 'websocket') {
			const coordinator = env.CONTAINER_COORDINATOR.get(env.CONTAINER_COORDINATOR.idFromName('global'));

			// Report connection opened
			ctx.waitUntil(
				coordinator
					.fetch(new URL('http://coordinator/connection-opened'), {
						method: 'POST',
						headers: { 'Content-Type': 'application/json' },
						body: JSON.stringify({ sessionId, containerId: containerInstanceId }),
					})
					.catch((error) => {
						console.error('Failed to report connection opened:', error);
					}),
			);

			// Note: We can't easily detect when WebSocket closes from here
			// The container would need to report back, or we'd need WebSocket interception
			// For now, rely on scale-down idle timeout
		}

		// Forward request directly to container
		return container.fetch(request);

	},
} satisfies ExportedHandler<Env>;

// Export the ContainerCoordinator Durable Object for auto-scaling
export { ContainerCoordinator } from './ContainerCoordinator';
